---
title: "Ejercicios_Estimacion_lineal"
author: "David Caceres"
date: "28/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
require(faraway)

```

## Ejercicios del libro de Faraway

### 1. (Ejercicio 1 cap. 2 pág. 30)

The dataset teengamb concerns a study of teenage gambling in Britain. Fit a regression model
with the expenditure on gambling as the response and the sex, status, income and verbal score as
predictors. Present the output.

```{r}
data("teengamb")
head(teengamb)
# Convertimos los datos de la columna "sex" en cualitativos como un factor y hacemos un resumen de los datos

teengamb$sex<-factor(teengamb$sex)
levels(teengamb$sex)<-c("male","female")
summary(teengamb)
```


```{r}
# Creamos el modelo lineal y lo presentamos
lmod<-lm(gamble ~ sex + status + income + verbal, data=teengamb)
summary(lmod)
```

#### (a) What percentage of variation in the response is explained by these predictors?

```{r}
# El porcentaje de variación de la respuesta es R²

lmodsum<-summary(lmod)
lmodsum

#R² puede extraerse del resumen de lmod
names(lmodsum)

# Lo piden en porcentaje
lmodsum$r.squared*100
```

#### (b) Which observation has the largest (positive) residual? Give the case number.

```{r}
e<-lmodsum$residuals
max(e)

# Posición
which.max(e)
```

#### (c) Compute the mean and median of the residuals.

```{r}
mean(e)
median(e)
```

#### (d) Compute the correlation of the residuals with the fitted values.

```{r}
cor(lmod$residuals,lmod$fitted.values)
```

#### (e) Compute the correlation of the residuals with the income.

```{r}
cor(lmod$residuals, teengamb$income)
```


#### (f) For all other predictors held constant, what would be the difference in predicted expenditure
on gambling for a male compared to a female?

```{r}
# Nos piden el coeficiente de regresión para la varaible sex
lmod$coefficients["sexfemale"]
```


### Ejercicio 2
The dataset uswages is drawn as a sample from the Current Population Survey in 1988. Fit a model with weekly wages as the response and years of education and experience as predictors. Report and give a simple interpretation to the regression coefﬁcient for years of education. Now ﬁt the same model but with logged weekly wages. Give an interpretation to the regression coefﬁcient for years of education. Which interpretation is more natural?

```{r}
data(uswages, package = "faraway")
head(uswages)
```


```{r}
# Creamos el modelo lineal
lmod2<-lm(wage ~ educ + exper, data = uswages)
summary(lmod2)
lmod2$coefficients

```

El modelo obtenido se podría expresar de la siguiente forma:

wague = -242.799412 + 51.175268 * educ + 9.774767 * exper

El salario se expresa por semanas, pero la educación y la experiencia se expresan por años.
Este modelo nos muestra que el salario se incrementará en 51,17 dólares por año de educación y en 9.77 dólares por año de experiencia.

```{r}
# Construimos el mismo modelo con los ingresos en forma logaritmoca

waguelog<-lm(log2(wage) ~ educ + exper, data=uswages)
summary(waguelog)
waguelog$coefficients
```

Obtenemos un segundo modelo que respondería a la siguiente expresión:

log2wague = 6.70899224 + 0.13057296 * educ + 0.02608184 * exper

En este caso, los coeficientes expresan la relación entre el aumento de una unidad en la variable respuesta y el aumento de las explicativas. Por ejemplo:

```{r}
1/0.13057
```

Se necesitarían 7,6 años de educación para doblar el salario (log de base 2).

- La primera expresión parace algo más sencilla de entender y da una referencia algo más clara, aunque los dos modelos son válidos.


### 4. (Ejercicio 4 cap. 2 pág. 30)
The dataset prostate comes from a study on 97 men with prostate cancer who were due to receive a
radical prostatectomy. Fit a model with lpsa as the response and lcavol as the predictor. Record
the residual standard error and the R 2 . Now add lweight, svi, lbph, age, lcp, pgg45 and gleason
to the model one at a time. For each model record the residual standard error and the R 2 . Plot
the trends in these two statistics.

```{r}
data(prostate, package="faraway")
head(prostate)

lmod_p<-lm(lpsa ~ lcavol, data = prostate)
summary(lmod_p)

```

```{r}
# El error residual estandar y R²
lmod_psum<-summary(lmod_p)
names(lmod_psum)

lmod_psum$sigma
lmod_psum$r.squared

```

```{r}
# Añadimos weight
lmod_p2<-lm(lpsa ~ lcavol + lweight, data = prostate)
lmod_psum2<-summary(lmod_p2)
lmod_psum2$sigma
lmod_psum2$r.squared
```


```{r}
# Añadimos svi
lmod_p3<-lm(lpsa ~ lcavol + lweight + svi, data = prostate)
lmod_psum3<-summary(lmod_p3)
lmod_psum3$sigma
lmod_psum3$r.squared

```

```{r}
# Añadimos lbph
lmod_p4<-lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate)
lmod_psum4<-summary(lmod_p4)
lmod_psum4$sigma
lmod_psum4$r.squared
```

```{r}
# Añadimos age

lmod_p5<-lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)
lmod_psum5<-summary(lmod_p5)
lmod_psum5$sigma
lmod_psum5$r.squared
```

```{r}
# Añadimos lcp

lmod_p6<-lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)
lmod_psum6<-summary(lmod_p6)
lmod_psum6$sigma
lmod_psum6$r.squared
```

```{r}
# Añadimos pgg45
lmod_p7<-lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + 
              pgg45, data = prostate)
lmod_psum7<-summary(lmod_p7)
lmod_psum7$sigma
lmod_psum7$r.squared
```

```{r}
# Añadimos gleason
lmod_p8<-lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + 
              pgg45 + gleason, data = prostate)
lmod_psum8<-summary(lmod_p8)
lmod_psum8$sigma
lmod_psum8$r.squared
```

```{r}
# Creamos un data frame con cada añadido
datos<-data.frame(
"Variables"=c(1,2,3,4,5,6,7,8),
"Error residual"=c(lmod_psum$sigma, lmod_psum2$sigma, lmod_psum3$sigma, lmod_psum4$sigma
,lmod_psum5$sigma, lmod_psum6$sigma, lmod_psum7$sigma, lmod_psum8$sigma),
"R²"=c(lmod_psum$r.squared, lmod_psum2$r.squared, lmod_psum3$r.squared, lmod_psum4$r.squared,
lmod_psum5$r.squared, lmod_psum6$r.squared, lmod_psum7$r.squared, lmod_psum8$r.squared))
datos
```


```{r}
matplot(datos[2:3],type ="b", xlab="Variables", ylab = "Valores", 
        col=c("blue","red")) 
legend("topright", legend=c("Error Residual", "R²"), fill =c("blue","red"))

# Vemos que conforme vamos añadiendo varaibles a la regresión, el error standar 
# y r² tienden a confluir poco a poco, es decir el error residual disminuye 
# mientras el ajuste es cada vez mejor. 
# Esto se debe a que conforme añadimos variables, y estando estas variables
# correlacionadas con la variable respuesta, a mayor número de datos, menor 
# varibilidad en los errores y mejor es el ajuste a la recta.
```


### 5. (Ejercicio 5 cap. 2 pág. 30)
Using the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa on lcavol and
lcavol on lpsa. Display both regression lines on the plot. At what point do the two lines intersect?
```{r}

recta1<-lm(lpsa ~ lcavol, data=prostate)
recta2<-lm(lcavol ~ lpsa, data=prostate)
recta1
recta2

```


Para comparar ambas rectas tenemos que invertir una de ellas, ya que si tomamos la ecuación de la recta, y= a + b * x para la recta1, para la recta2 la x y la y deben intercambiarse, de ese modo:

 x = a' + b' * y
 y = (x - a'/b)
 y = -a'/b' + 1/b' * x
 
```{r}
plot(lpsa ~ lcavol, data =prostate)
abline(recta1, col="red")
a_2<-recta2$coefficients[1]
b_2<-recta2$coefficients[2]
a2<- -a_2/b_2
b2<- 1/b_2
abline(a2,b2, col="blue")
```
 
### 6. (Ejercicio 6 cap. 2 pág. 30)
Thirty samples of cheddar cheese were analyzed for their content of acetic acid, hydrogen sulfide
and lactic acid. Each sample was tasted and scored by a panel of judges and the average taste score
produced. Use the cheddar data to answer the following:

```{r}
data(cheddar)
head(cheddar)
```

#### (a) Fit a regression model with taste as the response and the three chemical contents as predictors.
Report the values of the regression coefficients.

```{r}
cheddarlm<-lm(taste ~ Acetic + H2S + Lactic, data=cheddar)
cheddarlm$coefficients
```


#### (b) Compute the correlation between the fitted values and the response. Square it. Identify where
this value appears in the regression output.
```{r}
attach(cheddar)
correlacion<-cor(fitted.values(cheddarlm),taste)
correlacion^2
summary(cheddarlm)

# El cuadrado de la correlación coincide con el coeficiente de determinación o R² 
```


#### (c) Fit the same regression model but without an intercept term. What is the value of R 2 reported
in the output? Compute a more reasonable measure of the good- ness of fit for this example.

```{r}
cheddarlm2<-lm(taste ~ 0 + Acetic + H2S + Lactic, data = cheddar)
summary(cheddarlm2)
```

```{r}
# Sacamos el R²
cheddarlm2sum<-summary(cheddarlm2)
names(cheddarlm2sum)
cheddarlm2sum$r.squared

# El ajuste en este segundo modelo es mejor que en el primero a priori, pero 
# tenemos que tener en cuenta que cuando no se usa el intecepto, las fórmulas 
# para clacular R² varían y no son comparables.
```


#### (d) Compute the regression coefficients from the original fit using the QR decomposition showing
your R code.

```{r}
x<-model.matrix(~ Acetic + H2S + Lactic, data = cheddar)
y<- cheddar$taste
qrx<-qr(x)
dim(qr.Q(qrx))

(f<-t(qr.Q(qrx))%*%y)

backsolve(qr.R(qrx),f)
```


```{r}
# Los resultados coinciden con el modelo ajustado por mínimos cuadrados
summary(cheddarlm)
```

### 7. (Ejercicio 7 cap. 2 pág. 31)
An experiment was conducted to determine the effect of four factors on the resistivity of a semi-
conductor wafer. The data is found in wafer where each of the four factors is coded as − or +
depending on whether the low or the high setting for that factor was used. Fit the linear model
resist ~ x1 + x2 + x3 + x4.

```{r}
data(wafer, package = "faraway")
head(wafer)
waferlm<-lm(resist ~ x1 + x2 + x3 + x4, data = wafer)
summary(waferlm)
```

#### (a) Extract the X matrix using the model.matrix function. Examine this to determine how the
low and high levels have been coded in the model.

```{r}
mx<-model.matrix(waferlm)
mx

# Parece que los valores altos corresponden a los 1 o + en el dataset, y 
# los más bajos a los 0 o -.
```

#### (b) Compute the correlation in the X matrix. Why are there some missing values in the matrix?
```{r}
cor(mx)
# Hay valores perdidos porque corresponden a la intersección con el intercepto que tiene 1 como valor
```


#### (c) What difference in resistance is expected when moving from the low to the high level of x1?

```{r}
# Nos piden el coeficiente de regresión de x1
waferlm$coefficients
# Vemos que la resistencia es 25.76 veces mayor cuando los valores son altos
```

#### (d) Refit the model without x4 and examine the regression coefficients and standard errors? What
stayed the the same as the original fit and what changed?

```{r}
waferlm4<-lm(resist ~ x1 + x2 + x3, data = wafer)
summary(waferlm4)
```


```{r}
# Comparamos
summary(waferlm4)
summary(waferlm)

```

Vemos que los coeficientes no varían al eliminar x4 del modelo, probablemente debido a que son variables o experiencias independientes. Además x4 tiene un valor negativo, con lo cual afecta negativamente a la resistencia total, verificamos esto ya que al eliminar esta variable, la resistencia aumenta.

(e) Explain how the change in the regression coefficients is related to the correlation matrix of X.

La matriz contiene muchos ceros, por lo tanto, la covarianza entre x4 y los valores de x1, x2, x3, es cero. Por eso al quitar x4 del modelo, los demás coeficientes no varían.


## Ejercicios de libro de Carmona

### Ejercicio 2.1

Una variable Y toma los valores y1, y2 y y3 en función de otra variable X con los valores x1, x2 y x3. Determinar cuales de los siguientes modelos son lineales y encontrar, en su caso, la matriz de diseño para x1=1, x2=2 y x3=3.

#### a) yi = β0 + β1xi + β2 (x2i − 1) +  ei

```{r}
# Es un modelo lineal

matriz_a<-matrix(c(1,1,(1^2-1),1,2,(2^2-1),1,3,(3^2-1)), nrow = 3, 
                 byrow = T)
matriz_a
```

```{r}
# Es un modelo lineal
matriz_b<-matrix(c(1,1,exp(1),1,2,exp(2),1,3,exp(3)), nrow = 3, byrow = T)
matriz_b
```

```{r}
# No es lineal
```

### 2. (Ejercicio 2.4 del Capítulo 2 página 42)
Cuatro objetos cuyos pesos exactos son β 1 , β 2 , β 3 y β 4 han sido pesados en una balanza de platillos
de acuerdo con el siguiente esquema:

```{r}
# Nos están dando la matriz de diseño y los valores de la variable respuesta.
# Construimos la matriz y sus respuestas

x<-matrix(c(1,1,1,1,1,-1,1,1,1,0,0,1,1,0,0,-1,1,0,1,1,1,1,-1,1), nrow=6, byrow=T)
x

y<-c(9.2,8.3,5.4,-1.6,8.7,3.5)
    
```

#### Hallar las estimaciones de cada β i y de la varianza del error.

```{r}
# Primero calculamos las betas
solve(crossprod(x,x),crossprod(x,y))
```

```{r}
# La varianza del error a partir del modelo lineal sin intercept
x1 <- x[,1]; x2 <- x[,2]; x3 <- x[,3]; x4 <- x[,4]
lmodpesos2<-lm(y ~ 0 + x1 + x2 + x3 + x4)
lmodsumpesos2<-summary(lmodpesos2)
names(lmodsumpesos2)
lmodsumpesos2$sigma^2
```


# Ejercicios opcionales
## Faraway

### 3. (∗) (Ejercicio 3 cap. 2 pág. 30)
In this question, we investigate the relative merits of methods for computing the coefficients. Generate some artificial data by:

```{r}
x <- 1:20
y <- x+rnorm(20)
```

Fit a polynomial in x for predicting y. Compute β̂ in two ways — by lm() and by using the direct cal-
culation described in the chapter. At what degree of polynomial does the direct calculation method
fail? (Note the need for the I() function in fitting the polynomial, that is, lm(y ~ x + I(x^2)).

```{r}
# Construimos la recta con el polinomiony hayamos los coeficientes
recta2<-lm(y ~ x + I(x^2))
coef(recta2)
```


```{r}
# Pide también el método directo
matriz<-model.matrix(recta2)
solve(t(matriz) %*% matriz) %*% t(matriz) %*% y
```

Para 3

```{r}
recta3<-lm(y ~ x + I(x^2) + I(x^3))
coef(recta3)
```


```{r}
matriz<-model.matrix(recta3)
cbind(coef(recta3),solve(t(matriz) %*% matriz) %*% t(matriz) %*% y)
```

Para 4

```{r}
recta4<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4))
coef(recta4)
```


```{r}
matriz<-model.matrix(recta4)
cbind(coef(recta4),solve(t(matriz) %*% matriz) %*% t(matriz) %*% y)
```

Para 5

```{r}
recta5<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5))
coef(recta5)
```


```{r}
matriz<-model.matrix(recta5)
cbind(coef(recta5),solve(t(matriz) %*% matriz) %*% t(matriz) %*% y)
```

Para 6

```{r}
recta6<-lm(y ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5) + I(x^6))
coef(recta6)
```


```{r}
matriz<-model.matrix(recta6)
```

```{r eval=FALSE, include=FALSE}
cbind(coef(recta6),solve(t(matriz) %*% matriz) %*% t(matriz) %*% y)
```


### 8. (∗) (Ejercicio 8 cap. 2 pág. 31)
An experiment was conducted to examine factors that might affect the height of leaf springs in the
suspension of trucks. The data may be found in truck. The five factors in the experiment are set
to − and + but it will be more convenient for us to use −1 and +1. This can be achieved for the
first factor by:

```{r}
truck$B <- sapply(truck$B, function(x) ifelse(x == "-",-1,1))
truck$C <- sapply(truck$C, function(x) ifelse(x == "-",-1,1))
truck$D <- sapply(truck$D, function(x) ifelse(x == "-",-1,1))
truck$E <- sapply(truck$E, function(x) ifelse(x == "-",-1,1))
truck$O <- sapply(truck$O, function(x) ifelse(x == "-",-1,1))
```
#### (a) Fit a linear model for the height in terms of the five factors. Report on the value of the
regression coefficients.

```{r}
lmodt<-lm(height ~ B + C + D + E + O, data = truck)
lmodt$coefficients
```

#### (b) Fit a linear model using just factors B, C, D and E and report the coefficients. How do these com-
pare to the previous question? Show how we could have anticipated this result by examining
the X matrix.

```{r}
lmodt2<- lm(height ~ B + C + D + E, data = truck)
lmodt2$coefficients
# Los coeficientes son idénticos al modelo anterior, lo cual es lógico teniendo
# en cuenta que "O" tiene todos los valores negativos y no influye sobre el peso

# Comprobamos la matriz x
x<-model.matrix(lmodt)
t(x) %*% x

# Vemos que hay ortogonalidad, y que los coeficientes son independientes.
```

#### (c) Construct a new predictor called A which is set to B+C+D+E. Fit a linear model with the
predictors A, B, C, D, E and O. Do coefficients for all six predictors appear in the regression
summary? Explain.

```{r}
attach(truck)
A<-B+C+D+E
lmodt3<-lm(height ~ A + B + C + D + E + O, data = truck)
lmodt3$coefficients

# En este modelo, una de las variables es combinación de otras, así que el
# rango de la matriz de diseño no es máximo y no podemos calcular todas las betas
```


#### (d) Extract the model matrix X from the previous model. Attempt to compute β̂ from (X 0 X) −1 X 0 y.
What went wrong and why?

```{r}
x<-model.matrix(lmodt3)
```

```{r eval=FALSE, include=FALSE}
solve(t(x) %*% x)

# La matriz no es de rango máximo y no tiene inversa

```

#### (e) Use the QR decomposition method as seen in Section 2.7 to compute β̂. Are the results satisfactory?

```{r}
qrx<-qr(x)
(f <- t(qr.Q(qrx)) %*% height)
backsolve(qr.R(qrx),f)

# Los coeficientes no son iguales y tienen valores muy altos, la descomposición
# QR no va bien aquí
```

#### (f) Use the function qr.coef to correctly compute β̂.

```{r}
qr.coef(qrx, height)
```


## Ejercicios del libro de Carmona

### 3. (∗) (Ejercicio 3.2 del Capítulo 3 página 54)
En un modelo lineal, la matriz de diseño es


1 1 1 1 1
1 0 1 0 0
1 1 1 0 0
1 0 1 1 1


Hallar la expresión general de las funciones paramétricas estimables

```{r}
# Creamos la matriz y comprobamos su rango
matriz<-matrix(c(1,1,1,1,1,1,0,1,0,0,1,1,1,0,0,1,0,1,1,1),byrow = T, ncol=5)
qr(matriz)$rank

# Es de rango 3, con lo cual no es máximo
```

 Una función paramétrica puede estimarse si es combinación lineal de las filas 
 de la matriz de diseño. Tomamos 3 filas independientes.
 
```{r}
qr(matriz[1:3,])$rank
#Sigue siendo de rango 3
```
 
 
 Planteamos las combinaciones lineales:
 
 (a,b,c,d,e) = λ1(1,1,1,1,1) + λ2 (1,0,1,0,0) + λ3 (1,1,1,0,0)
 
 Y las ecuaciones lineales:

a = λ1 + λ2 + λ3
b = λ1 + λ3
c = λ1 + λ2 + λ3
d = λ1
e = λ1

Como el número de ecuaciones necesarios es igual al numero de columnas menos el 
rango 5-3= 2

Las ecuaciones paramétricas estimables cumplirán;

a=c y d=e


### Ejercicio 3.7
Consideremos el modelo lineal

y 1 = β 1 + β 2 + ϵ1
y 2 = β 1 + β 3 + ϵ2
y 3 = β 1 + β 2 + ϵ3


1) ¿Es la función paramétrica ψ = β1 + β2 + β3 estimable?

```{r}

matriz<-matrix(c(1,1,0,1,0,1,1,1,0),byrow=T,ncol = 3)
matriz
qr(matriz)$rank
# La matriz es de rango 2

matriz2<-matrix(c(1,1,0,1,0,1,1,1,0,1,1,1),byrow=T,ncol = 3)
matriz2
qr(matriz2)$rank

# Hemos añadido la función problema a la matriz, añadiendo sus coeficientes
# Vemos que el rango pasa a 3 y no se conserva, así que no sería estimable
```


2) Probar que toda función paramétrica

ψ = a1 β1 + a2 β2 + a3 β3

es estimable si y sólo si a1 = a2 + a3.

Partiendo de la misma matriz que en el apartado anterior, tomamos 2 filas 
independientes para rango 2

(a1,a2,a3) =λ1 (1,1,0) + λ2(1,0,1)

Sistema de ecuaciones:

a1 =  λ1 + λ2
a2 =  λ1
a3 =  λ2

El resultado verifica que a1 = a2 + a3



### Ejercicio 3.8

Consideremos el modelo lineal
y1 = µ + α1 + β1 + ϵ1
y2 = µ + α1 + β2 + ϵ2
y3 = µ + α2 + β1 + ϵ3
y4 = µ + α2 + β2 + ϵ4
y5 = µ + α3 + β1 + ϵ5
y6 = µ + α3 + β2 + ϵ6


(a) ¿Cuando es a0µ + a1 α1 + a2 α2 + a3 α3 + a4 β1 + a5 β2 estimable?


```{r}
# Creamos la matriz y vemos su rango

matriz<-matrix(c(1,1,0,0,1,0,
                 1,1,0,0,0,1,
                 1,0,1,0,1,0,
                 1,0,1,0,0,1,
                 1,0,0,1,1,0,
                 1,0,0,1,0,1), byrow = T, ncol = 6)
qr(matriz)$rank
```

Es de rango 4, y vemos que la suma de las columnas 2-4 es igual a la primera,
lo mismo pasa con la 5ª y 6ª.

```{r}
# Buscamos 4 columnas linealmente independientes

qr(matriz[1:4,])$rank
# No
qr(matriz[c(1,2,5,6)])$rank
# No
qr(matriz[c(1,2,4,5),])$rank
# Si

```



(a0,a1,a2,a3,a4,a5) = λ1(1,1,0,0,1,0) + λ2(1,1,0,0,0,1) + λ3(1,0,1,0,1,0) + 
λ4(1,0,0,1,1,0)

a0 = λ1 + λ2 + λ3 + λ4
a1 = λ1 + λ2
a2 = λ3
a3 = λ4
a4 = λ1 + λ3 + λ4
a5 = λ2

Necesitamos 6-4 = 2 ecuaciones

Así que:

a0 = a1 + a2 + a3

a0 = a4 + a5


a0µ + a1 α1 + a2 α2 + a3 α3 + a4 β1 + a5 β2 es estimable cuando se cumplen las 
2 ecuaciones anteriores.


#### (b) ¿Es α 1 + α 2 estimable?

Para esta función los coeficientes son: (a0,a1,a2,a3,a4,a5)= (0,1,1,0,0,0)

Usando las ecuaciones anteriores

a0 = a1 + a2 + a3

a0 = a4 + a5

Vemos que 

0 = 1 + 1 + 0, no se cumple, así que no es estimable


#### (c) ¿Es β 1 − β 2 estimable?

Para esta función los coeficientes son: (a0,a1,a2,a3,a4,a5)= (0,0,0,0,1,-1)

0 = 0 + 0 + 0
0 = 1 -1

Sería estimable

#### (d) ¿Es µ + α 1 estimable?

Para esta función los coeficientes son: (a0,a1,a2,a3,a4,a5)= (1,1,0,0,0,0)


1 = 1 + 0 + 0
1 = 0 + 0 

No estimable

#### (e) ¿Es 6µ + 2α1 + 2α2 + 2α3 + 3β1 + 3β2 estimable?

Para esta función los coeficientes son: (a0,a1,a2,a3,a4,a5)= (6,2,2,2,3,3)

6 = 2 + 2 + 2
6 = 3 + 3

Es estimable

#### (f) ¿Es α1 − 2α2 + α3 estimable?

Para esta función los coeficientes son: (a0,a1,a2,a3,a4,a5)= (0,1,-2,1,0,0)

0 = 1 - 2 + 1
0 = 0 + 0

Es estimable

#### (g) Hallar la covarianza entre los estimadores lineales MC de las funciones paramétricas β1 − β2 y α1 − α2

Comprobamos que la segunda función sea estimable, la primera ya lo es.

(0,1,-1,0,0,0,0)

0 = 1 - 1 + 0
0 = 0 + 0

Es estimable

La matriz de covarianzas es igual a:

Σ = cov(β) = σ2 (X  X)⁻

De este modo, podemos calcular la covarianza:

cov(β1−β2,α1 − α2) = cov(β1,α1) − cov(β1,α2) − cov(β2,α1) + cov(β 2 , α2 ) = σ52 − σ53 − σ62 + σ63

```{r}
library(MASS)
ginvxtx<-ginv(t(matriz) %*% matriz)
ginvxtx[5,2] - ginvxtx[5,3] - ginvxtx[6,2] + ginvxtx[6,2]
```

#### (h) Hallar la dimensión del espacio paramétrico.

La dimensión del espacio paramétrico es igual al rango de la matriz de diseño

```{r}
qr(matriz[c(1,2,4,5),])$rank
```

#### (i) Obtener una expresión del espacio de los errores.


### Ejercicio 3.10
Un transportista realiza diversos trayectos entre tres poblaciones A, B y C. En cuatro días consecutivos
ha hecho los recorridos que muestra la siguiente tabla:

trayecto
A → B → A → C
C → A → C → B
B → C → A → C → A → B → A
A → B → A → C → A → B → A

km
533
583
1111
1069
donde el kilometraje es, por diversas razones, aproximado.


#### (a) Proponer un modelo lineal, con la matriz de diseño y las hipótesis necesarias, para estimar las
distancias kilométricas entre las tres poblaciones.
Con los datos proporcionados, ¿es posible estimar las distancias entre las tres poblaciones? ¿Cuales son
las distancias o funciones paramétricas estimables (fpe) en este modelo?

Partiendo de la idea de que la distancia de una ciudad a otra es la misma en los 
dos trayectos tenemos:

 α = d(A,B)
 β = d(A,C)
 γ = d(B,C)
 
 El modelo lineal es:
 
 533 = 2α + β + ε1
 583 = 2β + γ + ε2
1111 = 2α + 3β + γ + ε3
1069 = 4α + 2β + ε4


```{r}
# Creamos la matriz y vemos su rango
matriz<-matrix(c(2,1,0,
0,2,1,
2,3,1,
4,2,0), byrow = T, ncol = 3)
qr(matriz)$rank

# Vemos que es de rango 2
```

Buscamos las funciones paramétricas estimables

(a1,a2,a3) = λ1(2,1,0) + λ2(0,2,1)

a1 = 2λ1
a2 = λ1 + 2λ2
a3 = λ2

4-2 = 2 ecuaciones

a2 = a1/2 + 2a3
2a2 = a1 + 4a3


Es posible estimar las distacias?

-d(A,B) -- (1,0,0) --- 2 * 0 = 1 + 4 * 0
No es estimable

-d(A,C) -- (0,1,0) --- 2 * 1 = 0 + 4 * 0 
No es estimable

-d(B,C) -- (0.0,1) --- 2 * 0 = 0 + 4 * 1
No es estimable


#### (b) ¿Se puede estimar el kilometraje del trayecto M BC → B → A → C → M AC , donde M IJ es el punto
medio entre dos poblaciones? ¿Es una buena estimación? ¿Cual es el error de esta estimación?

Lo traducimos al modelo:

1/2γ + α + β + 1/2β = α + 3/2β + 1/2γ

Vemos que:

2 * 3/2 = 1 + 4 * 1/2

Es correcto así que es estimable

```{r}
# Construimos el modelo
y<-c(533,583,1111,1069)
x<-matrix(c(2,1,0,
         0,2,1,
         2,3,1,
         4,2,0), byrow = T, ncol = 3)

```

```{r}
# Estimamos el valor
library(MASS)
betas<-ginv(t(x) %*% x) %*% t(x) %*% y
a<-c(1,3/2,1/2)
t(a) %*% betas
```


```{r}
# Caculamos el error de estimación

n<-length(y)
r<-2
residuos<-y-x %*% betas
sigma2<-sum(residuos^2)/(n-r)
var.error<-sigma2 * t(a) %*% ginv(t(x) %*% x) %*% a
error<-sqrt(var.error)
error
```

Vemos que el error es del 0,21%. Muy pequeño, la estimación es bastante buena