---
title: "Ejercicios_Estimacion_lineal"
author: "David Caceres"
date: "28/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
require(faraway)

```

## Ejercicios del libro de Faraway

### 1. (Ejercicio 1 cap. 2 pág. 30)

The dataset teengamb concerns a study of teenage gambling in Britain. Fit a regression model
with the expenditure on gambling as the response and the sex, status, income and verbal score as
predictors. Present the output.

```{r}
data("teengamb")
head(teengamb)
# Convertimos los datos de la columna "sex" en cualitativos como un factor y hacemos un resumen de los datos

teengamb$sex<-factor(teengamb$sex)
levels(teengamb$sex)<-c("male","female")
summary(teengamb)
```


```{r}
# Creamos el modelo lineal y lo presentamos
lmod<-lm(gamble ~ sex + status + income + verbal, data=teengamb)
summary(lmod)
```

(a) What percentage of variation in the response is explained by these predictors?

```{r}
# El porcentaje de variación de la respuesta es R²

lmodsum<-summary(lmod)
lmodsum

#R² puede extraerse del resumen de lmod
names(lmodsum)

# Lo piden en porcentaje
lmodsum$r.squared*100
```

(b) Which observation has the largest (positive) residual? Give the case number.

```{r}
e<-lmodsum$residuals
max(e)

# Posición
which.max(e)
```

(c) Compute the mean and median of the residuals.

```{r}
mean(e)
median(e)
```

(d) Compute the correlation of the residuals with the fitted values.

```{r}
cor(lmod$residuals,lmod$fitted.values)
```

(e) Compute the correlation of the residuals with the income.

```{r}
cor(lmod$residuals, teengamb$income)
```


(f) For all other predictors held constant, what would be the difference in predicted expenditure
on gambling for a male compared to a female?

```{r}
# Nos piden el coeficiente de regresión para la varaible sex
lmod$coefficients["sexfemale"]
```


### Ejercicio 2
The dataset uswages is drawn as a sample from the Current Population Survey in 1988. Fit a model with weekly wages as the response and years of education and experience as predictors. Report and give a simple interpretation to the regression coefﬁcient for years of education. Now ﬁt the same model but with logged weekly wages. Give an interpretation to the regression coefﬁcient for years of education. Which interpretation is more natural?

```{r}
data(uswages, package = "faraway")
head(uswages)
```


```{r}
# Creamos el modelo lineal
lmod2<-lm(wage ~ educ + exper, data = uswages)
summary(lmod2)
lmod2$coefficients

```

El modelo obtenido se podría expresar de la siguiente forma:

wague = -242.799412 + 51.175268 * educ + 9.774767 * exper

El salario se expresa por semanas, pero la educación y la experiencia se expresan por años.
Este modelo nos muestra que el salario se incrementará en 51,17 dólares por año de educación y en 9.77 dólares por año de experiencia.

```{r}
# Construimos el mismo modelo con los ingresos en forma logaritmoca

waguelog<-lm(log2(wage) ~ educ + exper, data=uswages)
summary(waguelog)
waguelog$coefficients
```

Obtenemos un segundo modelo que respondería a la siguiente expresión:

log2wague = 6.70899224 + 0.13057296 * educ + 0.02608184 * exper

En este caso, los coeficientes expresan la relación entre el aumento de una unidad en la variable respuesta y el aumento de las explicativas. Por ejemplo:

```{r}
1/0.13057
```

Se necesitarían 7,6 años de educación para doblar el salario (log de base 2).

- La primera expresión parace algo más sencilla de entender y da una referencia algo más clara, aunque los dos modelos son válidos.


### 4. (Ejercicio 4 cap. 2 pág. 30)
The dataset prostate comes from a study on 97 men with prostate cancer who were due to receive a
radical prostatectomy. Fit a model with lpsa as the response and lcavol as the predictor. Record
the residual standard error and the R 2 . Now add lweight, svi, lbph, age, lcp, pgg45 and gleason
to the model one at a time. For each model record the residual standard error and the R 2 . Plot
the trends in these two statistics.

```{r}
data(prostate, package="faraway")
head(prostate)

lmod_p<-lm(lpsa ~ lcavol, data = prostate)
summary(lmod_p)

```

```{r}
# El error residual estandar y R²
lmod_psum<-summary(lmod_p)
names(lmod_psum)

lmod_psum$sigma
lmod_psum$r.squared

```

```{r}
# Añadimos weight
lmod_p2<-lm(lpsa ~ lcavol + lweight, data = prostate)
lmod_psum2<-summary(lmod_p2)
lmod_psum2$sigma
lmod_psum2$r.squared
```


```{r}
# Añadimos svi
lmod_p3<-lm(lpsa ~ lcavol + lweight + svi, data = prostate)
lmod_psum3<-summary(lmod_p3)
lmod_psum3$sigma
lmod_psum3$r.squared

```

```{r}
# Añadimos lbph
lmod_p4<-lm(lpsa ~ lcavol + lweight + svi + lbph, data = prostate)
lmod_psum4<-summary(lmod_p4)
lmod_psum4$sigma
lmod_psum4$r.squared
```

```{r}
# Añadimos age

lmod_p5<-lm(lpsa ~ lcavol + lweight + svi + lbph + age, data = prostate)
lmod_psum5<-summary(lmod_p5)
lmod_psum5$sigma
lmod_psum5$r.squared
```

```{r}
# Añadimos lcp

lmod_p6<-lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp, data = prostate)
lmod_psum6<-summary(lmod_p6)
lmod_psum6$sigma
lmod_psum6$r.squared
```

```{r}
# Añadimos pgg45
lmod_p7<-lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + 
              pgg45, data = prostate)
lmod_psum7<-summary(lmod_p7)
lmod_psum7$sigma
lmod_psum7$r.squared
```

```{r}
# Añadimos gleason
lmod_p8<-lm(lpsa ~ lcavol + lweight + svi + lbph + age + lcp + 
              pgg45 + gleason, data = prostate)
lmod_psum8<-summary(lmod_p8)
lmod_psum8$sigma
lmod_psum8$r.squared
```

```{r}
# Creamos un data frame con cada añadido
datos<-data.frame(
"Variables"=c(1,2,3,4,5,6,7,8),
"Error residual"=c(lmod_psum$sigma, lmod_psum2$sigma, lmod_psum3$sigma, lmod_psum4$sigma
,lmod_psum5$sigma, lmod_psum6$sigma, lmod_psum7$sigma, lmod_psum8$sigma),
"R²"=c(lmod_psum$r.squared, lmod_psum2$r.squared, lmod_psum3$r.squared, lmod_psum4$r.squared,
lmod_psum5$r.squared, lmod_psum6$r.squared, lmod_psum7$r.squared, lmod_psum8$r.squared))
datos
```


```{r}
matplot(datos[2:3],type ="b", xlab="Variables", ylab = "Valores", 
        col=c("blue","red")) 
legend("topright", legend=c("Error Residual", "R²"), fill =c("blue","red"))

# Vemos que conforme vamos añadiendo varaibles a la regresión, el error standar 
# y r² tienden a confluir poco a poco, es decir el error residual disminuye 
# mientras el ajuste es cada vez mejor. 
# Esto se debe a que conforme añadimos variables, y estando estas variables
# correlacionadas con la variable respuesta, a mayor número de datos, menor 
# varibilidad en los errores y mejor es el ajuste a la recta.
```


### 5. (Ejercicio 5 cap. 2 pág. 30)
Using the prostate data, plot lpsa against lcavol. Fit the regressions of lpsa on lcavol and
lcavol on lpsa. Display both regression lines on the plot. At what point do the two lines intersect?


